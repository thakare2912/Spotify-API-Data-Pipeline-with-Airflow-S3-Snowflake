# üéµ Spotify End-to-End Data Pipeline using Apache Airflow, AWS & Snowflake

![Spotify Data Pipeline](![Spotify End-To-End Data Pipeline Project using airfleo](https://github.com/user-attachments/assets/a506bd49-14d4-43bd-a4be-34d80c41ca46)
)

## üìå Project Overview

This project demonstrates an **end-to-end ETL data pipeline** that extracts data from the **Spotify API**, stores raw and processed data in **Amazon S3**, and loads transformed data into **Snowflake** using **Snowpipe**, all orchestrated with **Apache Airflow** running in **Docker**.

---

## ‚öôÔ∏è **Architecture**

- **Source:** Spotify API  
- **Ingestion:** Python + Airflow tasks  
- **Storage:** Amazon S3 (raw + transformed data)  
- **Processing:** Python (Pandas) transformations  
- **Orchestration:** Apache Airflow on Docker  
- **Data Warehouse:** Snowflake (loaded via Snowpipe)

**üîó Note:** In the diagram, **Power BI** is not used ‚Äî analytics can be done directly in Snowflake or with other BI tools if needed.

---

## üóÇÔ∏è **Key Steps**

1Ô∏è‚É£ **Extract Data:**  
Fetch playlist data from the Spotify API using Spotipy and store raw JSON in Amazon S3.

2Ô∏è‚É£ **Transform Data:**  
Read raw JSON files from S3, process albums, artists, and songs data into clean CSVs using Pandas.

3Ô∏è‚É£ **Store Transformed Data:**  
Save transformed CSVs back to S3 in organized folders (`album_data`, `artist_data`, `songs_data`).

4Ô∏è‚É£ **Load into Snowflake:**  
Use Snowpipe to automatically ingest the transformed data from S3 into Snowflake for analysis.

5Ô∏è‚É£ **Automation:**  
All tasks are orchestrated with Apache Airflow running inside Docker **on your local machine**.

---

## üêç **Key Technologies**

- **Python**, Spotipy, Pandas
- **Apache Airflow**
- **Docker**
- **AWS S3**
- **Snowflake**, Snowpipe

---

